First set up the environment :
conda create -p venv python==3.12 -y
conda activate venv/

Update requirements.txt file with the required libraries 
and their versions specified.

--> Run : pip install -r requirements.txt

--> Run : pip install ipykernel

Select kernel and start coding in ipynb file.

Remember to delete venv folder before uploading on github.

End to End Deep Learning Project - ANN

CHURN MODELLING DATASET => CLASSIFICATION
=> (Perform Basic Feature Engineering) which involves :
     -> Convert Categorical variables to Numerical
     -> Standardize/Normalize the data

--> Then Build an ANN model using TensorFlow/Keras

--> Dropout Regularization to avoid Overfitting

--> Then in forward propagation we calculate the loss and Then
we will use some kind of optimizers (everything we will be doing using TensorFlow/Keras)

--> Finally we will do Backpropagation to update the weights and biases

--> Once the training is complete we will convert the model to some file format preferably a pickle file or h5 file format.

--> After this we will use Streamlit to build a web app where we will load this trained model and use this model to make predictions based on user input.

--> Finally we will deploy this web app using Streamlit cloud.






Uploading large files to Github repo:
GitHub has a file size limit of 100 MB for individual files. If you try to upload a file larger than this limit, you'll receive an error message. To upload large files to GitHub, you can use Git Large File Storage (Git LFS). Here's how to do it:


--> git lfs install

--> git lfs track "*.h5"

--> git lfs push --all origin main

--> git add .

--> git commit -m "Adding large file using Git LFS"

--> git push origin main
